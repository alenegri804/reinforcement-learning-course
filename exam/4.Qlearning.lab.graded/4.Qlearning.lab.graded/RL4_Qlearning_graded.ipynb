{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab 4 (graded)\n",
    "### J. Martinet\n",
    "\n",
    "Implement Q-learning from scratch\n",
    "\n",
    "Duration: 90 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) First version with a 1D grid world\n",
    "\n",
    "\n",
    "We have discussed Q-learning during the class. As you know, it is an off-policy algorithm that uses the Time Difference $\\delta_t$, which is the difference between the estimated value of $s_t$ and the better estimate $r_{t+1} + \\gamma V^\\pi (s_{t+1})$\n",
    "\n",
    "$$ \\delta_t = r_{t+1} + \\gamma V^\\pi (s_{t+1}) - V^\\pi (s_t) $$\n",
    "\n",
    "The general definition of Q-learning update rule is:\n",
    "\n",
    "$$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha[ r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t,a_t) ] $$\n",
    "\n",
    "\n",
    "In this part, we are going to implement Q-learning in the simple setting of a 1D grid world:\n",
    "\n",
    "![1D grid world](RL4_1dgrid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand:\n",
    "- the size of the grid world (= number of states)\n",
    "- the size of the action space (= number of possible actions)\n",
    "- the size of the Q-table\n",
    "- the expected reward for reaching each state\n",
    "\n",
    "The first step will be to initialize an empty Q-table, a table of rewards, a move cost, and alpha and gamma parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random, randint\n",
    "\n",
    "# we have 2 actions : move left and move right\n",
    "nb_action = 2\n",
    "nb_state = 6\n",
    "\n",
    "# we create a matrix 6*2 to represent the value of each move at a given state\n",
    "QTable = np.zeros((nb_state,nb_action))\n",
    "\n",
    "# the tab with the representation of the 6 states (-1 for the bad end, 1 for the good end, and 0 for other states)\n",
    "REWARD = [-1,0,0,0,0,1 ]\n",
    "\n",
    "# cost of one move\n",
    "cost = 0.01\n",
    "\n",
    "# learning rate - should not be too high, e.g. between .5 and .9\n",
    "alpha = 0.9\n",
    "\n",
    "# discount factor that shows how much you care about future (remember 0 for myopic)\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the interesting part. You need to write the main Q-learning loop.\n",
    "\n",
    "The first version will simply iterate:\n",
    "- choose an action (by looking up in the Q-table! Choose the most interesting move)\n",
    "- move\n",
    "- update the Q-table\n",
    "\n",
    "When you get this version, you can make it more complete to add the exploration/exploitation with the $\\epsilon$-greedy version, by initializing an $\\epsilon = 1$ that you decrease by e.g. 0.01 in each iteration.\n",
    "\n",
    "In your main loop, start by drawing a random number. If it is lower that $\\epsilon$, then EXPLORE (= take a random move), otherwise EXPLOIT (= choose the best move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "EPSILON_START = 1\n",
    "EPSILON_DECAY = 0.01\n",
    "EPISODES = 100\n",
    "\n",
    "def step(state, QTable, epsilon):\n",
    "    if random() < epsilon:\n",
    "        action_idx = randint(0, nb_action - 1)\n",
    "    else:\n",
    "        action_idx = np.argmax(QTable[state])\n",
    "\n",
    "    if action_idx == 0:\n",
    "        action = -1\n",
    "    else:\n",
    "        action = 1\n",
    "\n",
    "    next_state = state + action\n",
    "    next_state = max(0, min(next_state, nb_state - 1))\n",
    "\n",
    "    act_reward = REWARD[next_state] - cost\n",
    "\n",
    "    isTerminated = False\n",
    "    if REWARD[next_state] == -1 or REWARD[next_state] == 1:\n",
    "        isTerminated = True\n",
    "\n",
    "    return action_idx, next_state, act_reward, isTerminated\n",
    "\n",
    "def qLearning(QTable):\n",
    "    epsilon = EPSILON_START\n",
    "    for episode in range(EPISODES):\n",
    "        state = 2\n",
    "\n",
    "        i = 1\n",
    "        while True:\n",
    "            # print(f\"Episode {episode}: iteration {i}\")\n",
    "            action_idx, next_state, act_reward, isTerminated = step(state, QTable, epsilon)\n",
    "            old_val = QTable[state][action_idx]\n",
    "            next_max = max(QTable[next_state])\n",
    "\n",
    "            new_val = old_val + alpha * (act_reward + gamma * next_max - old_val)\n",
    "            QTable[state][action_idx] = new_val\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if isTerminated:\n",
    "                break\n",
    "                \n",
    "        epsilon -= EPSILON_DECAY\n",
    "        epsilon = max(0, epsilon)\n",
    "\n",
    "    return QTable\n",
    "    \n",
    "def main():\n",
    "    Q = qLearning(QTable)\n",
    "\n",
    "    print(\"\\nOutput legend:\")\n",
    "    print(\"State | Best action\")\n",
    "    actions = [\"L\", \"R\"]\n",
    "    for s in range(nb_state):\n",
    "        best = np.argmax(Q[s])\n",
    "        print(f\" {s} | {actions[best]} |\")\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Second version with a 2D grid world\n",
    "\n",
    "Same exercise, in the following 2D grid:\n",
    "\n",
    "![2D grid world](RL4_2dgrid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output\n",
      "|  R |  R |  R | GOAL |\n",
      "|  U | WALL |  U | TRAP |\n",
      "|  U |  R |  U |  L |\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "from random import random, randint\n",
    "\n",
    "ROWS = 3\n",
    "COLS = 4\n",
    "nb_action = 4\n",
    "\n",
    "QTable = np.zeros((ROWS, COLS, nb_action))\n",
    "WALL = (1, 1)\n",
    "\n",
    "REWARD_MAP = np.zeros((ROWS, COLS))\n",
    "REWARD_MAP[0, 3] = 1   \n",
    "REWARD_MAP[1, 3] = -1 \n",
    "\n",
    "cost = 0.01\n",
    "alpha = 0.9\n",
    "gamma = 0.5\n",
    "\n",
    "EPSILON_START = 1\n",
    "EPSILON_DECAY = 0.01\n",
    "EPISODES = 100\n",
    "\n",
    "def step(state, QTable, epsilon):\n",
    "    row, col = state\n",
    "\n",
    "    if random() < epsilon:\n",
    "        action_idx = randint(0, nb_action - 1)\n",
    "    else:\n",
    "        action_idx = np.argmax(QTable[row][col])\n",
    "\n",
    "    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    move_r, move_c = actions[action_idx]\n",
    "\n",
    "    next_r = row + move_r\n",
    "    next_c = col + move_c\n",
    "    next_r = max(0, min(next_r, ROWS - 1))\n",
    "    next_c = max(0, min(next_c, COLS - 1))\n",
    "\n",
    "    if (next_r, next_c) == WALL:\n",
    "        next_r = row\n",
    "        next_c = col\n",
    "\n",
    "    next_state = (next_r, next_c)\n",
    "\n",
    "    act_reward = REWARD_MAP[next_r][next_c] - cost\n",
    "\n",
    "    isTerminated = False\n",
    "    if REWARD_MAP[next_r][next_c] == -1 or REWARD_MAP[next_r][next_c] == 1:\n",
    "        isTerminated = True\n",
    "\n",
    "    return action_idx, next_state, act_reward, isTerminated\n",
    "\n",
    "def qLearning(QTable):\n",
    "    epsilon = EPSILON_START\n",
    "    for episode in range(EPISODES):\n",
    "        state = (2, 0)\n",
    "\n",
    "        i = 1\n",
    "        while True:\n",
    "            # print(f\"Episode {episode}: iteration {i}\")\n",
    "            action_idx, next_state, act_reward, isTerminated = step(state, QTable, epsilon)\n",
    "            r = state[0]\n",
    "            c = state[1]\n",
    "            next_r = next_state[0]\n",
    "            next_c = next_state[1]\n",
    "\n",
    "            old_val = QTable[r][c][action_idx]\n",
    "            next_max = max(QTable[next_r][next_c])\n",
    "\n",
    "            new_val = old_val + alpha * (act_reward + gamma * next_max - old_val)\n",
    "            QTable[r][c][action_idx] = new_val\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "            if isTerminated:\n",
    "                break\n",
    "            \n",
    "        epsilon -= EPSILON_DECAY\n",
    "        epsilon = max(0, epsilon) \n",
    "\n",
    "    return QTable\n",
    "\n",
    "def main():\n",
    "    Q =qLearning(QTable)\n",
    "    print(\"\\nOutput Legend:\")\n",
    "    print(\"U: Up, D: Down, L: Left, R: Right\")\n",
    "\n",
    "    moves_symbols = {0: \"U\", 1: \"D\", 2: \"L\", 3: \"R\"}\n",
    "\n",
    "    for r in range(ROWS):\n",
    "        row_str = \"|\"\n",
    "        for c in range(COLS):\n",
    "            if (r, c) == WALL:\n",
    "                row_str += \" WALL |\"\n",
    "            elif (r, c) == (0, 3): # Goal\n",
    "                row_str += \" GOAL |\"\n",
    "            elif (r, c) == (1, 3): # Trap\n",
    "                row_str += \" TRAP |\"\n",
    "            else:\n",
    "                best_action = np.argmax(Q[r][c])\n",
    "                row_str += f\"  {moves_symbols[best_action]} |\"\n",
    "        print(row_str)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Optional third part (with bonus): plot the evolution of the total reward\n",
    "\n",
    "Make a plot of the evolution of the total reward after each epidode during the simulation / learning with different values of $\\gamma$, $\\alpha$, and $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "import numpy as np\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROWS = 3\n",
    "COLS = 4\n",
    "nb_action = 4\n",
    "WALL = (1, 1)\n",
    "\n",
    "REWARD_MAP = np.zeros((ROWS, COLS))\n",
    "REWARD_MAP[0, 3] = 1 \n",
    "REWARD_MAP[1, 3] = -1\n",
    "\n",
    "cost = 0.01\n",
    "\n",
    "def step(state, QTable, epsilon):\n",
    "    row, col = state\n",
    "\n",
    "    if random() < epsilon:\n",
    "        action_idx = randint(0, nb_action - 1)\n",
    "    else:\n",
    "        action_idx = np.argmax(QTable[row][col])\n",
    "\n",
    "    actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    move_r, move_c = actions[action_idx]\n",
    "\n",
    "    next_r = row + move_r\n",
    "    next_c = col + move_c\n",
    "\n",
    "    next_r = max(0, min(next_r, ROWS - 1))\n",
    "    next_c = max(0, min(next_c, COLS - 1))\n",
    "\n",
    "    if (next_r, next_c) == WALL:\n",
    "        next_r = row\n",
    "        next_c = col\n",
    "\n",
    "    next_state = (next_r, next_c)\n",
    "\n",
    "    act_reward = REWARD_MAP[next_r][next_c] - cost\n",
    "\n",
    "    isTerminated = False\n",
    "    if REWARD_MAP[next_r][next_c] == -1 or REWARD_MAP[next_r][next_c] == 1:\n",
    "        isTerminated = True\n",
    "\n",
    "    return action_idx, next_state, act_reward, isTerminated\n",
    "\n",
    "def run(alpha, gamma, epsilon_start, epsilon_decay, episodes):\n",
    "    QTable = np.zeros((ROWS, COLS, nb_action))Ã¹\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
